# -*- coding: utf-8 -*-
"""Script.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Jn0iy2pYWDRjNw_bG7PLVSaIRbavvDfG
"""

import torch
import torch.nn as nn
from preprocessing import tokenize, encode, vocab  # reuse preprocessing functions from training

class LSTMWithMetadataAttention(nn.Module):
    # Define exactly as in your training notebook.
    pass

# Load model
model = LSTMWithMetadataAttention(vocab_size=len(vocab), job_size=..., party_size=..., context_size=..., embed_matrix=...)
model.load_state_dict(torch.load('fake_news_model.pth', map_location=torch.device('cpu')))
model.eval()

def predict_fake_news(statement, job, party, context):
    tokens = torch.tensor([encode(tokenize(statement))])
    length = torch.tensor([len(tokens[0])])
    job_tensor = torch.tensor([job])
    party_tensor = torch.tensor([party])
    context_tensor = torch.tensor([context])

    with torch.no_grad():
        output = model(tokens, length, job_tensor, party_tensor, context_tensor)
        prob = torch.softmax(output, dim=1)
        prediction = torch.argmax(prob, dim=1).item()
    return prediction, prob.numpy()