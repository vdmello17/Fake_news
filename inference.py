# -*- coding: utf-8 -*-
"""Script.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Jn0iy2pYWDRjNw_bG7PLVSaIRbavvDfG
"""

import torch
import torch.nn as nn
import re
from collections import Counter
vocab = {"<pad>": 0, "<unk>": 1}
vocab.update({word: i + 2 for i, (word, _) in enumerate(counter.items())})


def tokenize(text):
    return re.sub(r"[^\w\s]", "", text.lower()).split()

def encode(tokens, max_len=100):
    ids = [vocab.get(token, vocab["<unk>"]) for token in tokens]
    return ids[:max_len]


class LSTMWithMetadataAttention(nn.Module):
    # Define exactly as in your training notebook.
    pass

# Load model
model = LSTMWithMetadataAttention(vocab_size=len(vocab), job_size=..., party_size=..., context_size=..., embed_matrix=...)
model.load_state_dict(torch.load('fake_news_model.pth', map_location=torch.device('cpu')))
model.eval()

def predict_fake_news(statement, job, party, context):
    tokens = torch.tensor([encode(tokenize(statement))])
    length = torch.tensor([len(tokens[0])])
    job_tensor = torch.tensor([job])
    party_tensor = torch.tensor([party])
    context_tensor = torch.tensor([context])

    with torch.no_grad():
        output = model(tokens, length, job_tensor, party_tensor, context_tensor)
        prob = torch.softmax(output, dim=1)
        prediction = torch.argmax(prob, dim=1).item()
    return prediction, prob.numpy()
